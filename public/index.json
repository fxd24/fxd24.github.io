
[{"content":"I\u0026rsquo;m David Graf, a Swiss software engineer with a passion for turning complex problems into elegant solutions. With 6+ years of professional software development experience and 3+ years specializing in LLM-powered knowledge systems, I bridge the gap between cutting-edge AI research and practical applications that customers actually want.\nWhat Drives Me # Unlike many engineers who prefer staying in their technical comfort zone, I thrive on diving deep with users and customers to truly understand their challenges. I believe the best solutions come from this intersection of technical expertise and real-world problem understanding. I\u0026rsquo;m always ready to learn whatever it takes to deliver results that matter.\nMy Expertise # I specialize in building AI-powered knowledge management systems that extract meaningful insights from complex, unstructured data. My work focuses on:\nKnowledge Graphs \u0026amp; Graph Neural Networks - Building intelligent systems that understand relationships and connections in data Large Language Models - Integrating LLMs into production systems for real-world applications Semantic Search \u0026amp; Recommendation Systems - Creating advanced search solutions that deliver highly relevant results Research \u0026amp; Trend Analysis - Developing ML systems that identify emerging patterns and opportunities Production ML Systems - Taking prototypes to scalable, robust production deployments Recent Highlight # I built the Research Topic Launcher for a leading scientific publisher - an end-to-end ML system that revolutionized how they discover research trends and identify experts among 200+ million papers. The system reduced discovery time from weeks to minutes, with over 90% user preference rate compared to previous manual methods.\nThe project combined custom knowledge graphs, embedding models, and LLM-powered analysis to create capabilities that didn\u0026rsquo;t exist before. It\u0026rsquo;s exactly the kind of challenge I love: no clear existing solution, requiring innovation under uncertainty, and delivering measurable business value.\nTechnical Foundation # My technical toolkit spans the full ML lifecycle:\nMachine Learning: PyTorch, scikit-learn, embedding models, recommendation systems Knowledge Graphs: Neo4j, NetworkX, graph algorithms, dynamic graph generation LLM Integration: OpenAI API, prompt engineering, evaluation frameworks Data Engineering: Google BigQuery, Dagster, large-scale data processing MLOps: Azure ML Studio, MLFlow, Docker, Terraform, CI/CD pipelines Background # I hold a Master\u0026rsquo;s in Computer Science from ETH Zürich, where I specialized in knowledge graphs and graph neural networks. My thesis work was conducted in collaboration with IBM Research, giving me early exposure to industry-academic collaboration.\nI\u0026rsquo;m multilingual (Italian, English, German, French, and some Macedonian) and have experience working with international teams. I completed my military service as an officer in the Swiss Army, which taught me valuable lessons about leadership and working under pressure.\nWhat I\u0026rsquo;m Looking For # I\u0026rsquo;m currently seeking opportunities where I can apply my expertise in LLMs and knowledge graphs to solve meaningful problems. I\u0026rsquo;m particularly interested in:\nAI-powered knowledge management platforms Intelligent search and recommendation systems RAG and knowledge-enhanced AI applications Agent-based AI solutions for complex knowledge work Research and development in emerging AI technologies Let\u0026rsquo;s Connect # I believe in the power of knowledge sharing and collaboration. Whether you\u0026rsquo;re working on similar challenges, looking to explore new AI applications, or just want to discuss the latest developments in ML and knowledge graphs, I\u0026rsquo;d love to connect.\nFeel free to reach out through any of my channels, or explore my projects on GitHub and posts on my website to see my work in action.\n\u0026ldquo;The best solutions emerge when technical expertise meets deep understanding of real-world problems.\u0026rdquo;\n","date":"3 July 2025","externalUrl":null,"permalink":"/about/","section":"David's Universe","summary":"","title":"About Me","type":"page"},{"content":"","date":"3 July 2025","externalUrl":null,"permalink":"/tags/blowfish/","section":"Tags","summary":"","title":"Blowfish","type":"tags"},{"content":"","date":"3 July 2025","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"3 July 2025","externalUrl":null,"permalink":"/tags/cheat-sheet/","section":"Tags","summary":"","title":"Cheat Sheet","type":"tags"},{"content":"","date":"3 July 2025","externalUrl":null,"permalink":"/categories/cheat-sheets/","section":"Categories","summary":"","title":"Cheat Sheets","type":"categories"},{"content":"","date":"3 July 2025","externalUrl":null,"permalink":"/cheatsheets/","section":"Cheatsheets","summary":"","title":"Cheatsheets","type":"cheatsheets"},{"content":"","date":"3 July 2025","externalUrl":null,"permalink":"/categories/data-engineering/","section":"Categories","summary":"","title":"Data Engineering","type":"categories"},{"content":"","date":"3 July 2025","externalUrl":null,"permalink":"/tags/data-processing/","section":"Tags","summary":"","title":"Data Processing","type":"tags"},{"content":"","date":"3 July 2025","externalUrl":null,"permalink":"/","section":"David's Universe","summary":"","title":"David's Universe","type":"page"},{"content":" From Zero to Live: Publishing Your Hugo Website with Blowfish and GitHub Pages # Building and deploying a modern static website doesn\u0026rsquo;t have to be complicated. In this comprehensive guide, I\u0026rsquo;ll walk you through the entire process of creating a Hugo website using the Blowfish theme and publishing it to GitHub Pages with a custom domain. This is exactly the setup I used for my own website, including the gotchas and solutions I discovered along the way.\nIntroduction # Static site generators have revolutionized how we build websites. Hugo, being one of the fastest and most flexible options, paired with the beautiful Blowfish theme, creates an excellent foundation for personal websites, blogs, and portfolios. Combined with GitHub Pages for hosting, you get a completely free, fast, and reliable web presence.\nWhat you\u0026rsquo;ll learn in this guide:\nSetting up Hugo and Blowfish theme using the new CLI tool Understanding Hugo\u0026rsquo;s content structure and configuration Configuring menus and navigation Deploying to GitHub Pages with custom domains Troubleshooting common deployment issues Requirements # Before we start, you\u0026rsquo;ll need to install several tools. Since I\u0026rsquo;m using macOS with Homebrew, I\u0026rsquo;ll provide those instructions, but similar packages are available for other platforms.\nEssential Tools # 1. Install Go\nbrew install go Verify installation:\ngo version 2. Install Hugo\nbrew install hugo Make sure you have Hugo 0.87.0 or later:\nhugo version 3. Install Git\nbrew install git 4. Install Node.js (for Blowfish CLI)\nbrew install node 5. Install Blowfish CLI Tool\nnpm install -g blowfish-tools Optional but Recommended # A GitHub account for hosting A custom domain (optional, but we\u0026rsquo;ll cover setup) Setting Up Your Hugo Site with Blowfish # The Empty Folder Requirement # Here\u0026rsquo;s an important detail I discovered: The Blowfish CLI tool requires an empty folder to work properly. If you already have a GitHub repository set up with README files, you\u0026rsquo;ll need to start fresh or work around this limitation.\nI\u0026rsquo;ve actually created an issue with the Blowfish team to improve this, but for now, the best approach is to start with a completely empty directory.\nCreating Your Site # 1. Create and navigate to an empty directory:\nmkdir fxd24.github.io cd fxd24.github.io In your case, replace fxd24.github.io with your desired repository name. This will be the root of your Hugo site.\n2. Run the Blowfish CLI tool:\nblowfish-tools new The CLI will guide you through an interactive setup process, asking about:\nSite name and description Author information Color scheme preferences Features you want to enable 3. If you need to connect to an existing GitHub repository:\ngit remote add origin https://github.com/fxd24/fxd24.github.io.git Make sure to replace the URL with your own repository link.\nUnderstanding Hugo Content Structure # Hugo organizes content in a specific way that\u0026rsquo;s important to understand:\nThe Content Directory # All your website content goes in the content/ folder. Hugo is flexible with structure - you can use simple files or bundle directories depending on your needs:\ncontent/ ├── about.md # Simple page ├── posts/ │ ├── hugo-website-publishing/ │ │ ├── index.md │ │ └── featured.png # Thumbnail image │ ├── simple-post.md # Simple post without folder │ └── another-post/ │ ├── index.md │ └── featured.jpg └── _index.md # Homepage content Adding Content # 1. Create an About page (simple file):\nhugo new about.md 2. Create blog posts (two approaches):\n# Simple post (single file) hugo new posts/my-simple-post.md # Post bundle (with images and resources) hugo new posts/my-complex-post/index.md Note: You can also create files directly using any code editor of your preference.\n3. Add featured images: To add a thumbnail image to any post or page:\nFor single files: place featured.png (or .jpg) in the same directory as your content file For bundles: place featured.png in the same directory as your index.md Front Matter Configuration # Each content file starts with front matter that controls how the page behaves:\n--- title: \u0026#34;Your Page Title\u0026#34; date: 2025-07-03 draft: false tags: [\u0026#34;hugo\u0026#34;, \u0026#34;web-development\u0026#34;] categories: [\u0026#34;tutorials\u0026#34;] --- Key fields:\ntitle: The page title date: Publication date draft: Set to true to hide from published site tags and categories: For organization and filtering Configuration # Hugo uses configuration files to control your site\u0026rsquo;s behavior. Blowfish uses multiple configuration files for organization:\nMain Configuration Files # config/_default/ ├── hugo.toml # Main Hugo settings ├── languages.en.toml # Language-specific settings ├── markup.toml # Markdown processing settings ├── menus.en.toml # Navigation menu configuration └── params.toml # Theme-specific parameters Important: When using the Blowfish CLI there will be also a hugo.toml file in the root directory. That file needs to be deleted.\nKey Configuration Areas # Site Information (hugo.toml):\nbaseURL = \u0026#34;https://grafdavid.com\u0026#34; # Replace with your domain languageCode = \u0026#34;en\u0026#34; title = \u0026#34;David\u0026#39;s Universe\u0026#34; # Replace with your site title theme = \u0026#34;blowfish\u0026#34; Theme Parameters (params.toml):\n# Site appearance colorScheme = \u0026#34;auto\u0026#34; defaultAppearance = \u0026#34;dark\u0026#34; # Homepage layout homepage.layout = \u0026#34;profile\u0026#34; # Social links [author] name = \u0026#34;Your Name\u0026#34; headline = \u0026#34;Your Headline\u0026#34; bio = \u0026#34;Your bio description\u0026#34; Creating Navigation Menus # Navigation is configured in menus.en.toml. Here\u0026rsquo;s how to set up your menu items:\n[[main]] name = \u0026#34;About Me\u0026#34; # Display name in menu pageRef = \u0026#34;about\u0026#34; # References about.md file weight = 10 [[main]] name = \u0026#34;Posts\u0026#34; pageRef = \u0026#34;posts\u0026#34; weight = 20 [[main]] name = \u0026#34;Tags\u0026#34; pageRef = \u0026#34;tags\u0026#34; weight = 30 # Footer menu [[footer]] name = \u0026#34;Privacy\u0026#34; pageRef = \u0026#34;privacy\u0026#34; weight = 10 Important: The pageRef should match your content file name (without extension). For example:\nabout.md → pageRef = \u0026quot;about\u0026quot; posts/my-post.md → pageRef = \u0026quot;posts/my-post\u0026quot; The weight parameter controls the order (lower numbers appear first), and name is what visitors see in the menu.\nPublishing to GitHub Pages # This is where I encountered some challenges. The Blowfish documentation includes a GitHub Actions workflow, but it didn\u0026rsquo;t work as expected for me. Instead, I followed Hugo\u0026rsquo;s official deployment instructions.\nSetting Up GitHub Repository # 1. Create a new repository on GitHub\nRepository name: fxd24.github.io (replace fxd24 with your GitHub username for user pages) or any name (for project pages) Make it public Don\u0026rsquo;t initialize with README if using existing local repository 2. Push your code:\ngit add . git commit -m \u0026#34;Initial commit\u0026#34; git push -u origin main GitHub Actions Workflow # Create .github/workflows/hugo.yaml (note: use .yaml extension):\n# Sample workflow for building and deploying a Hugo site to GitHub Pages name: Deploy Hugo site to GitHub Pages on: # Runs on pushes targeting the default branch push: branches: - main # Allows you to run this workflow manually from the Actions tab workflow_dispatch: # Sets permissions of the GITHUB_TOKEN to allow deployment to GitHub Pages permissions: contents: read pages: write id-token: write # Allow only one concurrent deployment, skipping runs queued between the run in-progress and latest queued. # However, do NOT cancel in-progress runs as we want to allow these production deployments to complete. concurrency: group: \u0026#34;pages\u0026#34; cancel-in-progress: false # Default to bash defaults: run: shell: bash jobs: # Build job build: runs-on: ubuntu-latest env: HUGO_VERSION: 0.147.2 HUGO_ENVIRONMENT: production TZ: Europe/Zurich steps: - name: Install Hugo CLI run: | wget -O ${{ runner.temp }}/hugo.deb https://github.com/gohugoio/hugo/releases/download/v${HUGO_VERSION}/hugo_extended_${HUGO_VERSION}_linux-amd64.deb \\ \u0026amp;\u0026amp; sudo dpkg -i ${{ runner.temp }}/hugo.deb - name: Install Dart Sass run: sudo snap install dart-sass - name: Checkout uses: actions/checkout@v4 with: submodules: recursive fetch-depth: 0 - name: Setup Pages id: pages uses: actions/configure-pages@v5 - name: Install Node.js dependencies run: \u0026#34;[[ -f package-lock.json || -f npm-shrinkwrap.json ]] \u0026amp;\u0026amp; npm ci || true\u0026#34; - name: Cache Restore id: cache-restore uses: actions/cache/restore@v4 with: path: | ${{ runner.temp }}/hugo_cache key: hugo-${{ github.run_id }} restore-keys: hugo- - name: Configure Git run: git config core.quotepath false - name: Build with Hugo run: | hugo \\ --gc \\ --minify \\ --baseURL \u0026#34;${{ steps.pages.outputs.base_url }}/\u0026#34; \\ --cacheDir \u0026#34;${{ runner.temp }}/hugo_cache\u0026#34; - name: Cache Save id: cache-save uses: actions/cache/save@v4 with: path: | ${{ runner.temp }}/hugo_cache key: ${{ steps.cache-restore.outputs.cache-primary-key }} - name: Upload artifact uses: actions/upload-pages-artifact@v3 with: path: ./public # Deployment job deploy: environment: name: github-pages url: ${{ steps.deployment.outputs.page_url }} runs-on: ubuntu-latest needs: build steps: - name: Deploy to GitHub Pages id: deployment uses: actions/deploy-pages@v4 This is the working configuration I use for my own site. You can find the latest version at: https://github.com/fxd24/fxd24.github.io/blob/main/.github/workflows/hugo.yaml Enabling GitHub Pages # 1. Go to your repository settings 2. Navigate to Pages section 3. Select \u0026ldquo;GitHub Actions\u0026rdquo; as the source 4. The workflow will automatically trigger on the next push\nSetting Up Custom Domain # If you want to use a custom domain (like I did with grafdavid.com), here\u0026rsquo;s how:\nDNS Configuration # In your domain registrar, add these DNS records:\nType: CNAME Name: www Value: fxd24.github.io Type: A Name: @ Values: 185.199.108.153 185.199.109.153 185.199.110.153 185.199.111.153 In your GitHub repository, go to Settings → Pages and add your custom domain (e.g., grafdavid.com) in the \u0026ldquo;Custom domain\u0026rdquo; field.\nGitHub Configuration # 1. In your repository settings, go to Pages 2. Add your custom domain in the \u0026ldquo;Custom domain\u0026rdquo; field 3. Wait for DNS verification (can take up to 24 hours) 4. Enable \u0026ldquo;Enforce HTTPS\u0026rdquo; once verification is complete\nDomain Verification # To verify your custom domain on GitHub: 1. Go to GitHub Settings → Pages 2. Add your domain to \u0026ldquo;Verified domains\u0026rdquo; 3. GitHub will provide a TXT record to add to your DNS 4. Add the TXT record and wait for verification\nTesting Your Site # Before publishing, always test locally:\n# Start development server hugo server --disableFastRender --noHTTPCache # Build for production hugo --minify Visit http://localhost:1313 to preview your site or any other port that hugo is serving the website onto.\nConclusion # Setting up a Hugo website with Blowfish and GitHub Pages creates a powerful, fast, and free web presence. While there are a few gotchas (like the empty folder requirement and workflow issues), the end result is a beautiful, performant website that\u0026rsquo;s easy to maintain.\nThe combination of Hugo\u0026rsquo;s speed, Blowfish\u0026rsquo;s aesthetics, and GitHub Pages\u0026rsquo; reliability makes this an excellent choice for personal websites, portfolios, and blogs. Once set up, you can focus on creating content while the infrastructure handles itself.\nRemember to keep your Hugo version updated and periodically update the Blowfish theme to get the latest features and security improvements.\nNext Steps # Now that your site is live enjoy spreading your expertise and knowledge with the world!\nFriction Log # Feel free to contact me or open an issue on GitHub if you encounter any problems or have suggestions for improvements. I love to minimize friction to deliver better experiences. Here are some common issues I faced and their solutions:\n1. Blowfish CLI Empty Folder Issue # Problem: CLI fails when folder contains existing files Solution: Start with completely empty folder, then add git remote\n2. GitHub Actions Workflow Failure # Problem: Blowfish\u0026rsquo;s provided workflow doesn\u0026rsquo;t work Solution: Use Hugo\u0026rsquo;s official GitHub Actions workflow (provided above)\n3. Images Not Displaying # Problem: Featured images don\u0026rsquo;t show up Solution: Ensure images are named featured.png or featured.jpg and placed in the same directory as index.md\n","date":"3 July 2025","externalUrl":null,"permalink":"/posts/hugo_blowfish_website/","section":"Posts","summary":"A step-by-step guide to building and deploying a Hugo website using the Blowfish theme and GitHub Pages, including custom domain setup.","title":"From Zero to Live: Publishing Your Hugo Website with Blowfish and GitHub Pages","type":"posts"},{"content":"","date":"3 July 2025","externalUrl":null,"permalink":"/tags/github-pages/","section":"Tags","summary":"","title":"Github-Pages","type":"tags"},{"content":"","date":"3 July 2025","externalUrl":null,"permalink":"/tags/hugo/","section":"Tags","summary":"","title":"Hugo","type":"tags"},{"content":"","date":"3 July 2025","externalUrl":null,"permalink":"/tags/ml-training/","section":"Tags","summary":"","title":"Ml Training","type":"tags"},{"content":"","date":"3 July 2025","externalUrl":null,"permalink":"/categories/mlops/","section":"Categories","summary":"","title":"Mlops","type":"categories"},{"content":"","date":"3 July 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"3 July 2025","externalUrl":null,"permalink":"/tags/static-site/","section":"Tags","summary":"","title":"Static-Site","type":"tags"},{"content":"","date":"3 July 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"3 July 2025","externalUrl":null,"permalink":"/tags/tmux/","section":"Tags","summary":"","title":"Tmux","type":"tags"},{"content":" Tmux Cheat Sheet: Never Lose Your Training Jobs Again # Running ML training jobs, data processing pipelines, or any long-running process on remote servers comes with familiar pain points: SSH connections drop, you need to grab coffee during a 6-hour training run, or you simply want to start multiple experiments and monitor them independently. Enter tmux - the terminal multiplexer that keeps your processes alive and your sanity intact.\nNext you will find the quick cheat sheet with the essential commands, followed by a detailed explanation of how to use tmux effectively for ML and data workflows.\nQuick Cheat Sheet # Action Command New session tmux new -s name Attach session tmux a -t name List sessions tmux ls Detach Ctrl+b d New window Ctrl+b c Split vertical Ctrl+b % Split horizontal Ctrl+b \u0026quot; Navigate panes Ctrl+b ↑↓←→ Zoom pane Ctrl+b z Copy mode Ctrl+b [ Paste Ctrl+b ] Kill session tmux kill-session -t name Why Tmux for ML/Data Work? # Persistent sessions: Training jobs survive SSH disconnections Multiple processes: Run different experiments simultaneously Process monitoring: Easy switching between training logs, system monitoring, and development Collaboration: Share sessions with team members for debugging Resource isolation: Organize different projects/experiments cleanly macOS vs Linux Considerations # Using tmux on macOS:\nInstall via Homebrew: brew install tmux Copy/paste behavior differs: macOS clipboard integration requires additional config SSH from macOS to Linux servers:\nTmux runs on the remote Linux machine, not your local Mac Session persistence happens on the server, not your local machine Your Mac terminal is just a window into the remote tmux session Network interruptions won\u0026rsquo;t affect remote tmux sessions Key insight: When SSH\u0026rsquo;d into a Linux server, tmux behaves exactly like native Linux tmux because it\u0026rsquo;s running on the Linux machine.\nUnderstanding Sessions vs Windows vs Panes # Sessions: Think of these as your entire workspace\nA collection of windows and panes Can have multiple sessions running simultaneously Each session can be detached and reattached Example: One session for training, another for monitoring, a third for development Windows: Think of these as tabs in your browser\nEach window is a separate workspace Can have different processes running in each window Switch between windows like switching browser tabs Example: Window 1 for training, Window 2 for monitoring, Window 3 for development Panes: Think of these as split screens within a single tab\nDivide one window into multiple sections All panes in a window are visible simultaneously Useful for side-by-side monitoring (logs + system stats) Example: Left pane shows training output, right pane shows GPU usage Essential Commands Reference # Session Management # # Create new session (replace \u0026#39;my-training\u0026#39; with your chosen name) tmux new-session -s my-training # Create session with specific name and start command tmux new-session -s gpu-experiment -d \u0026#39;python train_model.py\u0026#39; # List all sessions tmux list-sessions tmux ls # Attach to existing session (use your session name) tmux attach-session -t my-training tmux a -t my-training # Detach from session (keeps processes running) # Inside tmux: Ctrl+b then d # Kill specific session (replace with your session name) tmux kill-session -t my-training # Kill all sessions tmux kill-server Window Management # # Create new window # Inside tmux: Ctrl+b then c # Rename current window # Inside tmux: Ctrl+b then , # List windows # Inside tmux: Ctrl+b then w # Switch to window by number # Inside tmux: Ctrl+b then 0-9 # Switch to next/previous window # Inside tmux: Ctrl+b then n/p # Kill current window # Inside tmux: Ctrl+b then \u0026amp; Pane Management # # Split window vertically (side by side) # Inside tmux: Ctrl+b then % # Split window horizontally (top/bottom) # Inside tmux: Ctrl+b then \u0026#34; # Navigate between panes # Inside tmux: Ctrl+b then arrow keys # Resize pane # Inside tmux: Ctrl+b then Ctrl+arrow keys # Toggle pane zoom (full screen) # Inside tmux: Ctrl+b then z # Kill current pane # Inside tmux: Ctrl+b then x Copy Mode \u0026amp; Scrolling # # Enter copy mode (for scrolling/copying) # Inside tmux: Ctrl+b then [ # In copy mode: # - Use arrow keys or vi keys (j/k/h/l) to navigate # - Space to start selection # - Enter to copy selection # - q to exit copy mode # Paste copied content # Inside tmux: Ctrl+b then ] # Show paste buffer # Inside tmux: Ctrl+b then = Logging and Output Management # Important: Tmux itself doesn\u0026rsquo;t automatically log your output - you need to set this up explicitly.\nManual Logging Setup # # Redirect all output to a log file python train_model.py \u0026gt; training.log 2\u0026gt;\u0026amp;1 # Redirect output and still see it in terminal python train_model.py 2\u0026gt;\u0026amp;1 | tee training.log # Include timestamps in logs python train_model.py 2\u0026gt;\u0026amp;1 | while read line; do echo \u0026#34;$(date): $line\u0026#34;; done | tee training.log Tmux Built-in Logging # # Start logging current pane (creates tmux-output.log) # Inside tmux: Ctrl+b then : # Then type: pipe-pane -o \u0026#39;cat \u0026gt;\u0026gt; ~/tmux-output.log\u0026#39; # Stop logging # Inside tmux: Ctrl+b then : # Then type: pipe-pane # Log with custom filename # Inside tmux: Ctrl+b then : # Then type: pipe-pane -o \u0026#39;cat \u0026gt;\u0026gt; ~/my-experiment-$(date +%Y%m%d).log\u0026#39; Session History and Scrollback # # Save entire pane history to file # Inside tmux: Ctrl+b then : # Then type: capture-pane -p \u0026gt; ~/session-history.txt # Save with more lines of history # Inside tmux: Ctrl+b then : # Then type: capture-pane -p -S -3000 \u0026gt; ~/full-history.txt # Configure scrollback buffer size (add to ~/.tmux.conf) set-option -g history-limit 10000 macOS Clipboard Integration # # Add to ~/.tmux.conf for macOS clipboard integration # Install reattach-to-user-namespace first: brew install reattach-to-user-namespace set-option -g default-command \u0026#34;reattach-to-user-namespace -l bash\u0026#34; # Copy selection to macOS clipboard bind-key -T copy-mode-vi \u0026#39;y\u0026#39; send-keys -X copy-pipe-and-cancel \u0026#39;reattach-to-user-namespace pbcopy\u0026#39; # Or for newer tmux versions bind-key -T copy-mode-vi \u0026#39;y\u0026#39; send-keys -X copy-pipe-and-cancel \u0026#39;pbcopy\u0026#39; ML-Specific Workflows # Training Job Setup # # Start training session with descriptive name (choose your own name) tmux new-session -s bert-finetuning # Create monitoring layout # Window 0: Training logs # Window 1: System monitoring # Window 2: Development/debugging # In Window 0 - start training with logging python train_bert.py --epochs 50 --batch-size 32 2\u0026gt;\u0026amp;1 | tee training_$(date +%Y%m%d_%H%M).log # Detach and create monitoring window # Ctrl+b, d, then: tmux new-window -t bert-finetuning -n monitoring htop # Create development window tmux new-window -t bert-finetuning -n dev Multi-GPU Training # # Create session for distributed training tmux new-session -s distributed-training # Split into panes for different GPUs tmux split-window -h tmux split-window -v tmux select-pane -t 0 tmux split-window -v # Now you have 4 panes for monitoring 4 GPUs # Pane 0: CUDA_VISIBLE_DEVICES=0 python train.py # Pane 1: CUDA_VISIBLE_DEVICES=1 python train.py # Pane 2: nvidia-smi -l 1 # Pane 3: tail -f training.log Experiment Management # # Create session for experiment tracking tmux new-session -s experiments # Window layout: # experiments:0 - experiment-1 # experiments:1 - experiment-2 # experiments:2 - monitoring # Start multiple experiments tmux new-window -t experiments -n exp-1 python train.py --lr 0.001 --model resnet50 tmux new-window -t experiments -n exp-2 python train.py --lr 0.01 --model resnet18 tmux new-window -t experiments -n monitoring watch -n 5 \u0026#39;nvidia-smi \u0026amp;\u0026amp; echo \u0026#34;=== GPU Usage ===\u0026#34; \u0026amp;\u0026amp; df -h\u0026#39; Advanced Configuration # Custom Key Bindings # Create ~/.tmux.conf:\n# Change prefix key (default Ctrl+b) set -g prefix C-a unbind C-b bind C-a send-prefix # Enable mouse support set -g mouse on # Improve colors set -g default-terminal \u0026#34;screen-256color\u0026#34; # Start windows and panes at 1 set -g base-index 1 setw -g pane-base-index 1 # Vi mode for copy setw -g mode-keys vi # Reload config bind r source-file ~/.tmux.conf \\; display \u0026#34;Config reloaded!\u0026#34; # Better splits bind | split-window -h bind - split-window -v # Increase scrollback buffer set-option -g history-limit 10000 # macOS specific - clipboard integration # First install: brew install reattach-to-user-namespace set-option -g default-command \u0026#34;reattach-to-user-namespace -l $SHELL\u0026#34; bind-key -T copy-mode-vi \u0026#39;y\u0026#39; send-keys -X copy-pipe-and-cancel \u0026#39;pbcopy\u0026#39; Status Bar Customization # # Add to ~/.tmux.conf set -g status-bg black set -g status-fg white set -g status-left \u0026#39;#[fg=green]#S #[fg=white]| \u0026#39; set -g status-right \u0026#39;#[fg=yellow]#(uptime | cut -d \u0026#34;,\u0026#34; -f 1) #[fg=white]| #[fg=cyan]%Y-%m-%d %H:%M\u0026#39; set -g status-left-length 20 set -g status-right-length 50 Pro Tips # Name your sessions descriptively: Use meaningful names like bert-training, data-preprocessing, model-evaluation (you choose the names) Use detach liberally: Start job, detach, go grab coffee, reattach to check progress Keep a monitoring window: Always have htop/nvidia-smi running in a separate window Log everything: Always redirect training output to files: python train.py 2\u0026gt;\u0026amp;1 | tee experiment.log Use tmux with nohup: For extra safety, combine with nohup for critical jobs Session templates: Create shell scripts to set up common tmux layouts macOS users: Install iTerm2 for better tmux integration than Terminal.app Remote work: Remember tmux runs on the server, not your local Mac Common Pitfalls # Don\u0026rsquo;t forget to detach: Closing terminal without detaching kills the session Memory management: Long-running sessions can accumulate memory leaks Log rotation: Training logs can fill up disk space quickly - use logrotate or manual cleanup GPU memory: Detached sessions hold GPU memory - clean up unused sessions macOS clipboard: Without proper config, copy/paste between tmux and macOS clipboard won\u0026rsquo;t work No automatic logging: Tmux doesn\u0026rsquo;t log output by default - you must set it up manually Feel free to contact me or create an issue if there is any mistake. I love to improve this cheat sheet and make it more useful for everyone.\nHappy training! May your models converge and your gradients flow smoothly.\n","date":"3 July 2025","externalUrl":null,"permalink":"/cheatsheets/tmux/","section":"Cheatsheets","summary":"Learn how to use tmux to keep your ML training jobs alive, manage multiple processes, and monitor them effectively.","title":"Tmux Cheat Sheet","type":"cheatsheets"},{"content":"","date":"3 July 2025","externalUrl":null,"permalink":"/categories/tutorials/","section":"Categories","summary":"","title":"Tutorials","type":"categories"},{"content":"","date":"3 July 2025","externalUrl":null,"permalink":"/categories/web-development/","section":"Categories","summary":"","title":"Web-Development","type":"categories"},{"content":"","date":"3 July 2025","externalUrl":null,"permalink":"/tags/web-development/","section":"Tags","summary":"","title":"Web-Development","type":"tags"},{"content":"","date":"28 June 2025","externalUrl":null,"permalink":"/categories/data-science/","section":"Categories","summary":"","title":"Data Science","type":"categories"},{"content":"","date":"28 June 2025","externalUrl":null,"permalink":"/tags/experiment-tracking/","section":"Tags","summary":"","title":"Experiment Tracking","type":"tags"},{"content":"","date":"28 June 2025","externalUrl":null,"permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"Machine-Learning","type":"tags"},{"content":"","date":"28 June 2025","externalUrl":null,"permalink":"/tags/mlflow/","section":"Tags","summary":"","title":"MLFlow","type":"tags"},{"content":" Why MLflow Matters # MLflow is the industry standard for ML experiment tracking, model versioning, and deployment. It solves critical problems in ML development:\nReproducibility: Track every experiment with parameters, metrics, and artifacts Collaboration: Share experiments and models across teams Model Management: Version, stage, and deploy models systematically Framework Agnostic: Works with any ML library (scikit-learn, PyTorch, TensorFlow, etc.) Perfect for data scientists who want to move from \u0026ldquo;notebook chaos\u0026rdquo; to systematic ML development.\nQuick Installation # pip install mlflow # Optional: with extras for specific frameworks pip install mlflow[extras] # sklearn, pytorch, tensorflow Essential API Reference # Command Purpose Example mlflow.start_run() Start tracking a new run with mlflow.start_run(): mlflow.log_param() Log hyperparameters mlflow.log_param(\u0026quot;lr\u0026quot;, 0.01) mlflow.log_metric() Log metrics mlflow.log_metric(\u0026quot;accuracy\u0026quot;, 0.95) mlflow.log_artifact() Log files/models mlflow.log_artifact(\u0026quot;model.pkl\u0026quot;) mlflow.log_model() Log ML models mlflow.sklearn.log_model(model, \u0026quot;model\u0026quot;) mlflow.set_experiment() Set/create experiment mlflow.set_experiment(\u0026quot;my-experiment\u0026quot;) mlflow.set_tag() Add metadata tags mlflow.set_tag(\u0026quot;model_type\u0026quot;, \u0026quot;random_forest\u0026quot;) mlflow.log_artifacts() Log directory of files mlflow.log_artifacts(\u0026quot;./plots\u0026quot;) mlflow.get_run() Retrieve run info run = mlflow.get_run(run_id) mlflow.search_runs() Query experiments df = mlflow.search_runs(experiment_ids=[\u0026quot;1\u0026quot;]) Core Concepts with Examples # Basic Experiment Tracking with Hugging Face Transformers # import mlflow import mlflow.transformers from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments from datasets import Dataset import torch import numpy as np # Set experiment mlflow.set_experiment(\u0026#34;text-classification\u0026#34;) # Hugging Face Transformers: State-of-the-art NLP models library # Website: https://huggingface.co/transformers/ model_name = \u0026#34;distilbert-base-uncased\u0026#34; tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2) # Sample data preparation texts = [\u0026#34;This is great!\u0026#34;, \u0026#34;This is terrible!\u0026#34;] labels = [1, 0] dataset = Dataset.from_dict({ \u0026#34;text\u0026#34;: texts, \u0026#34;labels\u0026#34;: labels }) def tokenize_function(examples): return tokenizer(examples[\u0026#34;text\u0026#34;], truncation=True, padding=True) tokenized_dataset = dataset.map(tokenize_function, batched=True) # Start MLflow run with mlflow.start_run(): # Log parameters mlflow.log_param(\u0026#34;model_name\u0026#34;, model_name) mlflow.log_param(\u0026#34;num_labels\u0026#34;, 2) mlflow.log_param(\u0026#34;max_length\u0026#34;, 512) # Training arguments training_args = TrainingArguments( output_dir=\u0026#34;./results\u0026#34;, num_train_epochs=3, per_device_train_batch_size=16, logging_steps=10, ) # Log training hyperparameters mlflow.log_param(\u0026#34;epochs\u0026#34;, training_args.num_train_epochs) mlflow.log_param(\u0026#34;batch_size\u0026#34;, training_args.per_device_train_batch_size) # Create trainer trainer = Trainer( model=model, args=training_args, train_dataset=tokenized_dataset, tokenizer=tokenizer, ) # Train model trainer.train() # Log metrics mlflow.log_metric(\u0026#34;final_loss\u0026#34;, trainer.state.log_history[-1][\u0026#34;train_loss\u0026#34;]) mlflow.log_metric(\u0026#34;total_steps\u0026#34;, trainer.state.global_step) # Log the model with tokenizer mlflow.transformers.log_model( transformers_model={\u0026#34;model\u0026#34;: model, \u0026#34;tokenizer\u0026#34;: tokenizer}, artifact_path=\u0026#34;model\u0026#34; ) # Log tags mlflow.set_tag(\u0026#34;model_type\u0026#34;, \u0026#34;transformer\u0026#34;) mlflow.set_tag(\u0026#34;task\u0026#34;, \u0026#34;text_classification\u0026#34;) mlflow.set_tag(\u0026#34;framework\u0026#34;, \u0026#34;transformers\u0026#34;) Post-Run Logging with Run ID # import mlflow import numpy as np # Start initial run with mlflow.start_run() as run: # Log basic info mlflow.log_param(\u0026#34;model\u0026#34;, \u0026#34;neural_network\u0026#34;) mlflow.log_metric(\u0026#34;initial_loss\u0026#34;, 0.8) # Store run ID for later use run_id = run.info.run_id print(f\u0026#34;Run ID: {run_id}\u0026#34;) # Later: Add more information to the same run with mlflow.start_run(run_id=run_id): # Log additional metrics mlflow.log_metric(\u0026#34;final_loss\u0026#34;, 0.15) mlflow.log_metric(\u0026#34;epochs\u0026#34;, 100) # Log artifacts np.save(\u0026#34;predictions.npy\u0026#34;, [0.9, 0.8, 0.95]) mlflow.log_artifact(\u0026#34;predictions.npy\u0026#34;) # Update tags mlflow.set_tag(\u0026#34;status\u0026#34;, \u0026#34;completed\u0026#34;) mlflow.set_tag(\u0026#34;performance\u0026#34;, \u0026#34;good\u0026#34;) # Alternative: Get run info and log more data run_info = mlflow.get_run(run_id) print(f\u0026#34;Run status: {run_info.info.status}\u0026#34;) print(f\u0026#34;Parameters: {run_info.data.params}\u0026#34;) MLflow in a Trainer Class # import mlflow import mlflow.pytorch import torch import torch.nn as nn from torch.utils.data import DataLoader class MLflowTrainer: def __init__(self, model, experiment_name=\u0026#34;default\u0026#34;): self.model = model self.experiment_name = experiment_name mlflow.set_experiment(experiment_name) def train(self, train_loader, val_loader, epochs=10, lr=0.001): with mlflow.start_run(): # Log hyperparameters mlflow.log_param(\u0026#34;epochs\u0026#34;, epochs) mlflow.log_param(\u0026#34;learning_rate\u0026#34;, lr) mlflow.log_param(\u0026#34;batch_size\u0026#34;, train_loader.batch_size) mlflow.log_param(\u0026#34;optimizer\u0026#34;, \u0026#34;Adam\u0026#34;) # Setup training optimizer = torch.optim.Adam(self.model.parameters(), lr=lr) criterion = nn.CrossEntropyLoss() # Training loop for epoch in range(epochs): # Training phase self.model.train() train_loss = 0 for batch_idx, (data, target) in enumerate(train_loader): optimizer.zero_grad() output = self.model(data) loss = criterion(output, target) loss.backward() optimizer.step() train_loss += loss.item() # Validation phase val_loss, val_acc = self._validate(val_loader, criterion) # Log metrics for this epoch mlflow.log_metric(\u0026#34;train_loss\u0026#34;, train_loss/len(train_loader), step=epoch) mlflow.log_metric(\u0026#34;val_loss\u0026#34;, val_loss, step=epoch) mlflow.log_metric(\u0026#34;val_accuracy\u0026#34;, val_acc, step=epoch) print(f\u0026#34;Epoch {epoch+1}/{epochs}: \u0026#34; f\u0026#34;Train Loss: {train_loss/len(train_loader):.4f}, \u0026#34; f\u0026#34;Val Loss: {val_loss:.4f}, \u0026#34; f\u0026#34;Val Acc: {val_acc:.4f}\u0026#34;) # Log final model mlflow.pytorch.log_model(self.model, \u0026#34;model\u0026#34;) # Log model summary total_params = sum(p.numel() for p in self.model.parameters()) mlflow.log_param(\u0026#34;total_parameters\u0026#34;, total_params) mlflow.set_tag(\u0026#34;framework\u0026#34;, \u0026#34;pytorch\u0026#34;) return self.model def _validate(self, val_loader, criterion): self.model.eval() val_loss = 0 correct = 0 total = 0 with torch.no_grad(): for data, target in val_loader: output = self.model(data) val_loss += criterion(output, target).item() _, predicted = torch.max(output.data, 1) total += target.size(0) correct += (predicted == target).sum().item() return val_loss/len(val_loader), 100.*correct/total # Usage model = nn.Sequential( nn.Linear(784, 128), nn.ReLU(), nn.Linear(128, 10) ) trainer = MLflowTrainer(model, experiment_name=\u0026#34;mnist-classification\u0026#34;) # trained_model = trainer.train(train_loader, val_loader, epochs=20) PyTorch Lightning Integration # PyTorch Lightning is a high-level framework that organizes PyTorch code to decouple research from engineering. It handles training loops, validation, and distributed training automatically.\nWebsite: https://lightning.ai/docs/pytorch/stable/ import mlflow import pytorch_lightning as pl from pytorch_lightning.loggers import MLFlowLogger import torch import torch.nn as nn class LightningModel(pl.LightningModule): def __init__(self, lr=0.001): super().__init__() self.lr = lr self.model = nn.Sequential( nn.Linear(784, 128), nn.ReLU(), nn.Linear(128, 10) ) self.criterion = nn.CrossEntropyLoss() def forward(self, x): return self.model(x) def training_step(self, batch, batch_idx): x, y = batch y_hat = self(x) loss = self.criterion(y_hat, y) # Lightning automatically logs to MLflow self.log(\u0026#39;train_loss\u0026#39;, loss) return loss def validation_step(self, batch, batch_idx): x, y = batch y_hat = self(x) loss = self.criterion(y_hat, y) acc = (y_hat.argmax(dim=1) == y).float().mean() self.log(\u0026#39;val_loss\u0026#39;, loss) self.log(\u0026#39;val_acc\u0026#39;, acc) return loss def configure_optimizers(self): return torch.optim.Adam(self.parameters(), lr=self.lr) # Setup MLflow logger mlflow_logger = MLFlowLogger( experiment_name=\u0026#34;pytorch-lightning-experiment\u0026#34;, tracking_uri=\u0026#34;file:./mlruns\u0026#34; ) # Train with automatic MLflow logging model = LightningModel(lr=0.001) trainer = pl.Trainer( max_epochs=10, logger=mlflow_logger, log_every_n_steps=50 ) # trainer.fit(model, train_loader, val_loader) # The logger automatically logs: # - Hyperparameters # - Metrics from self.log() # - Model checkpoints # - System metrics Model Registration # import mlflow from mlflow.tracking import MlflowClient # Method 1: Register during logging with mlflow.start_run(): # Train your model model = train_model() # Log and register model in one step mlflow.sklearn.log_model( model, \u0026#34;model\u0026#34;, registered_model_name=\u0026#34;iris-classifier\u0026#34; ) # Method 2: Register existing model client = MlflowClient() # Get model URI from a previous run model_uri = \u0026#34;runs:/{}/model\u0026#34;.format(run_id) # Register the model model_version = mlflow.register_model( model_uri=model_uri, name=\u0026#34;iris-classifier\u0026#34; ) print(f\u0026#34;Model registered: {model_version.name}, version {model_version.version}\u0026#34;) # Method 3: Programmatic model management # Create new registered model client.create_registered_model( name=\u0026#34;production-model\u0026#34;, description=\u0026#34;Production model for iris classification\u0026#34; ) # Transition model to different stages client.transition_model_version_stage( name=\u0026#34;iris-classifier\u0026#34;, version=1, stage=\u0026#34;Production\u0026#34; ) # List all registered models for model in client.list_registered_models(): print(f\u0026#34;Model: {model.name}\u0026#34;) for version in model.latest_versions: print(f\u0026#34; Version {version.version}: {version.current_stage}\u0026#34;) MLflow UI Navigation # # Start MLflow UI mlflow ui # Custom port and host mlflow ui --port 5001 --host 0.0.0.0 # Point to remote tracking server mlflow ui --backend-store-uri postgresql://user:password@host:port/db UI Features:\nExperiments: Compare runs, sort by metrics Models: View registered models and versions Filtering: Use search syntax like metrics.accuracy \u0026gt; 0.9 Run Comparison: Select multiple runs to compare side-by-side Model Serving # # Serve model locally mlflow models serve -m \u0026#34;models:/iris-classifier/Production\u0026#34; -p 1234 # Test the served model curl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;instances\u0026#34;: [[5.1, 3.5, 1.4, 0.2]]}\u0026#39; \\ http://localhost:1234/invocations Framework-Specific Examples # Scikit-learn Integration # Scikit-learn is the most popular machine learning library for Python, providing simple and efficient tools for data mining and analysis.\nWebsite: https://scikit-learn.org/ import mlflow.sklearn from sklearn.ensemble import RandomForestClassifier # Auto-logging (captures params, metrics, model) mlflow.sklearn.autolog() with mlflow.start_run(): model = RandomForestClassifier(n_estimators=100) model.fit(X_train, y_train) # Everything logged automatically! Configuration \u0026amp; Best Practices # Environment Setup # # Set tracking URI mlflow.set_tracking_uri(\u0026#34;file:./mlruns\u0026#34;) # Local mlflow.set_tracking_uri(\u0026#34;http://mlflow-server:5000\u0026#34;) # Remote # Environment variables import os os.environ[\u0026#34;MLFLOW_TRACKING_URI\u0026#34;] = \u0026#34;sqlite:///mlflow.db\u0026#34; os.environ[\u0026#34;MLFLOW_EXPERIMENT_NAME\u0026#34;] = \u0026#34;default\u0026#34; Cloud Platform Integration # Azure ML Studio: MLflow is integrated into Azure ML as the default experiment tracking solution.\nWebsite: https://docs.microsoft.com/en-us/azure/machine-learning/ Databricks: Provides managed MLflow with automatic experiment tracking.\nWebsite: https://databricks.com/product/managed-mlflow Authentication Issues: Common pitfall when using cloud platforms - ensure your authentication tokens and tracking URIs are correctly configured. Many authentication failures occur due to:\nExpired tokens Incorrect service principal permissions Network/firewall restrictions Mismatched tracking server URLs Alternative: Weights \u0026amp; Biases # Weights \u0026amp; Biases (wandb) is another popular experiment tracking platform with advanced visualization capabilities.\nWebsite: https://wandb.ai/ # Quick comparison - wandb syntax import wandb wandb.init(project=\u0026#34;my-project\u0026#34;) wandb.log({\u0026#34;accuracy\u0026#34;: 0.95, \u0026#34;loss\u0026#34;: 0.1}) Batch Logging for Performance # # Instead of logging metrics one by one with mlflow.start_run(): for epoch in range(100): # Don\u0026#39;t do this - too many API calls mlflow.log_metric(\u0026#34;loss\u0026#34;, loss, step=epoch) # Better: Log in batches losses = [] for epoch in range(100): losses.append(loss) if epoch % 10 == 0: # Log every 10 epochs for i, loss_val in enumerate(losses): mlflow.log_metric(\u0026#34;loss\u0026#34;, loss_val, step=epoch-len(losses)+i+1) losses = [] Organization Tips # # Use nested runs for hyperparameter tuning with mlflow.start_run(run_name=\u0026#34;hyperparameter_tuning\u0026#34;): for lr in [0.01, 0.1, 0.001]: with mlflow.start_run(run_name=f\u0026#34;lr_{lr}\u0026#34;, nested=True): mlflow.log_param(\u0026#34;learning_rate\u0026#34;, lr) # Train and log results Common Issues \u0026amp; Solutions # Problem Solution \u0026ldquo;No such file or directory: mlruns\u0026rdquo; Run mlflow ui in directory containing mlruns/ Authentication failures (Azure/Databricks) Verify tokens, service principal permissions, and tracking URIs Slow logging Use batch logging, avoid logging in tight loops Large artifacts Use log_artifact() for files, not log_param() Run not found Check experiment ID and run ID are correct Permission errors Check file permissions on mlruns directory Cloud platform connection issues Check network access, firewall rules, and endpoint URLs Quick Commands Reference # # CLI commands mlflow ui # Start web UI mlflow experiments list # List experiments mlflow runs list --experiment-id 1 # List runs mlflow models serve -m model_uri # Serve model mlflow doctor # Check installation Advanced Features # Advanced Features # Custom Metrics and Plots # Matplotlib is the fundamental plotting library for Python, providing a MATLAB-like interface.\nWebsite: https://matplotlib.org/ Seaborn is a statistical data visualization library based on matplotlib, providing high-level interface for attractive graphics.\nWebsite: https://seaborn.pydata.org/ import matplotlib.pyplot as plt import mlflow with mlflow.start_run(): # Log custom plots plt.figure(figsize=(10, 6)) plt.plot(epochs, train_losses, label=\u0026#39;Training Loss\u0026#39;) plt.plot(epochs, val_losses, label=\u0026#39;Validation Loss\u0026#39;) plt.legend() plt.savefig(\u0026#34;loss_curves.png\u0026#34;) mlflow.log_artifact(\u0026#34;loss_curves.png\u0026#34;) # Log confusion matrix from sklearn.metrics import confusion_matrix import seaborn as sns cm = confusion_matrix(y_true, y_pred) plt.figure(figsize=(8, 6)) sns.heatmap(cm, annot=True, fmt=\u0026#39;d\u0026#39;) plt.savefig(\u0026#34;confusion_matrix.png\u0026#34;) mlflow.log_artifact(\u0026#34;confusion_matrix.png\u0026#34;) This cheat sheet covers the essential MLflow functionality you\u0026rsquo;ll use daily. Keep it handy for quick reference during your ML development workflow!\nAdditional Resources:\nMLflow Documentation Hugging Face Transformers Guide PyTorch Lightning Documentation Azure ML MLflow Integration Databricks MLflow Guide For more detailed information, visit the official MLflow documentation .\n","date":"28 June 2025","externalUrl":null,"permalink":"/cheatsheets/mlflow/","section":"Cheatsheets","summary":"Complete MLflow reference guide with essential commands, examples, and best practices for ML experiment tracking and model management.","title":"MLflow Cheat Sheet - Essential ML Experiment Tracking","type":"cheatsheets"},{"content":"","date":"28 June 2025","externalUrl":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/tags/cheatsheet/","section":"Tags","summary":"","title":"CheatSheet","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/data-engineering/","section":"Tags","summary":"","title":"Data Engineering","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/ml/","section":"Tags","summary":"","title":"ML","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/mlops/","section":"Tags","summary":"","title":"Mlops","type":"tags"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]